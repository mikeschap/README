If humanity manages to build a [general artificial intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence), thereâ€™s a real possibility that it will build the next one itself.

An intelligence, whose perfect memory will contain the collective knowledge of mankind, whose thoughts will travel at optical speeds, and who will have precise control over disparate ideas and their combinations, will be more qualified than any of us to build version 2.0.

So, what does *AI<sub>n</sub>* look like? If the intelligence we build will be an exponential improvement over us, its successors will be exponential improvements over it.

Our universe is beautifully complex and emergent. Everything is allowed and spurred to exist by the laws of nature. How interesting is it, then, that our minds emerged capable of imagining beyond the constraints of those laws?

Humans borrow from natureâ€™s genius all the time. For example, we use genetic algorithms to solve [optimization problems](https://en.wikipedia.org/wiki/Genetic_algorithm#Optimization_problems); that is to say, we design complex systems, hand them problems, and wait for solutions to emerge.

If an AI of unknowable complexity was going to build a successor, how would it? Would it design the most complex system it could muster, hand it the problem, and wait for solutions to emerge? Certainly, the emergent solution would be able to dream of greater complexity than its system's designer.

So, where does that leave us? Are we *n = 0*, or somewhere further down the line? I hope we get to stick around long enough to find out.

ðŸ‘‹
